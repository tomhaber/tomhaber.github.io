[{"authors":null,"categories":null,"content":"  Download my resumé.\n","date":1590969600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1590969600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Download my resumé.","tags":null,"title":"Tom Haber","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://tomhaber.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"Severo.jl was created for the scalable analysis of single-cell RNA-seq datasets: the implementation is designed to minimize hardware resources such as memory while maximizing performance through parallelism and hardware optimizations.\nSevero.jl gives an order of magnitude speedup when compared to other existing packages such as Seurat and Scanpy.\nThe package provides a toolbox of different algorithms and statistical methods from which the user can pick and choose.\n","date":1603411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603411200,"objectID":"ea7dbb9f7a9631561f2ff62dc2084ba9","permalink":"https://tomhaber.github.io/project/severo/","publishdate":"2020-10-23T00:00:00Z","relpermalink":"/project/severo/","section":"project","summary":"Software package for scalable analysis and exploration of single-cell RNA-seq datasets","tags":["single-cell","RNA-sequencing","High-Performance Computing","Parallel"],"title":"Severo.jl","type":"project"},{"authors":["Balazs Nemeth","Tom Haber","Jori Liesenborgs","Win Lamotte"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"773a3968595e8be029bc16756d028ea0","permalink":"https://tomhaber.github.io/publication/conditional/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/conditional/","section":"publication","summary":"Hierarchical models describe phenomena by grouping data into multiple levels. Due to the size of these models, parallel execution is required to avoid prohibitively long computing time. While it is occasionally possible to specify some of these models using parallel building blocks, this limits expressivity. Therefore, a more general generative specification is preferred. To leverage parallel computing capacity, these specifications can be annotated, but doing so effectively assumes that the modeler has expertise from computer science. This paper outlines how to identify parallel parts automatically by leveraging the conditional independence property in the graphical model extracted from the dataflow graph of model specifications. Computation related to random variables with the same depth in the graphical model are identified as candidates for parallel execution. Since subsequent proposals in the parameter space exploration of the model are clustered together, the results show that the well known longest processing time scheduling heuristic deals adequately with load imbalance. The proposed parallelization is evaluated on two pharmacometrics models, a domain where hierarchical models with load imbalance are common due to the numeric simulation of pharmacokinetics and pharmacodynamics of human subjects. The varying number of measurements taken per subject further exacerbates load imbalance.","tags":["High Performance Computing","Descriptive Language","Probabilistic Modelling","Automatic Parallelization","Dataflow","Hierarchical Models"],"title":"From Conditional Independence to Parallel Execution in Hierarchical Models","type":"publication"},{"authors":["Balazs Nemeth","Tom Haber","Jori Liesenborgs","Win Lamotte"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"6a0cf90bd760932dae5008113cc43ed7","permalink":"https://tomhaber.github.io/publication/autopar/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/autopar/","section":"publication","summary":"As scientists are designing increasingly complex and intricate models, the prominent way today to achieve acceptable execution time without sacrificing accuracy is through parallel computing. These techniques can improve execution time either on the level of the optimization methods or on the level of the model evaluations. This paper outlines an automatic parallelization approach for the latter. Processor specific procedures with embedded communication primitives are generated from static schedules produced by an evolutionary algorithm. These are passed to an optimizing compiler to avoid the overhead of typical task runtime systems. The two key insights are that the parallel structure of probabilistic models is revealed when the data is combined with the model and that static schedules can be combined into more robust schedules that can deal with varying load imbalance. For this, LogP model parameters and execution time of each computational task are measured and fed into a discrete event simulator to estimate the running time on the target parallel system. Performance is evaluated with three pharmacological models with different characteristics. The first model lacks enough exploitable parallelism while up to approximately 6x and 8x improvements are achieved for the other models. Compared to a theoretical system with infinite processors and no communication delay, this equates to exploiting 66% and 99% of the available parallelism. Performance improves even when load imbalance varies.","tags":["High Performance Computing","Descriptive Language","Probabilistic Modeling","Automatic Parallelization","Dataflow","LogP model","Simulation","Evolutionary Algorithms","Scheduling","Load Imbalance"],"title":"Automatic Parallelization of Probabilistic Models with Varying Load Imbalance","type":"publication"},{"authors":["Tom Haber","Frank van Reeth"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"44d8b8c0d9ce1b77d00300798e171a3d","permalink":"https://tomhaber.github.io/publication/improving/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/improving/","section":"publication","summary":"Non-linear mixed effects models (NLMEM) are frequently used in drug development for pharmacokinetic (PK) and pharmacokinetic-pharmacodynamic (PK-PD) analyses. Parameter estimation for these models can be time-consuming due to the need for numerical integration. Additionally, the structural model is often expressed using differential equations requiring computationally intensive time-stepping ODE solvers. Overall, this often leads to long computation times in the order of hours or even days. Combining the right mathematical tools as well as techniques from computer science, the computational cost can be significantly reduced. In this paper, several approaches are detailed for improving the performance of parameter estimation for NLMEM. Applying these, often easy, techniques can lead to an order of magnitude speedup.","tags":["Non-linear","Mixed effects models","High-Performance Computing","Parallel"],"title":"Improving the runtime performance of non-linear mixed-effects model estimation","type":"publication"},{"authors":["Tom Haber","Christian Fuchs","Philippe Bekaert","Hans-Peter Seidel","Michael Goesele","Hendrik P. A. Lensch"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"84e8bac671d949b21ad1797a46a29113","permalink":"https://tomhaber.github.io/publication/relighting/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/relighting/","section":"publication","summary":"We present an approach for recovering the reflectance of a static scene with known geometry from a collection of images taken under distant, unknown illumination. In contrast to previous work, we allow the illumination to vary between the images, which greatly increases the applicability of the approach. Using an all-frequency relighting framework based on wavelets, we are able to simultaneously estimate the per-image incident illumination and the per-surface point reflectance. The wavelet framework allows for incorporating various reflection models. We demonstrate the quality of our results for synthetic test cases as well as for several datasets captured under laboratory conditions. Combined with multi-view stereo reconstruction, we are even able to recover the geometry and reflectance of a scene solely using images collected from the Internet.","tags":["Rellighting","Image collections","Reflectance","multi-view stereo"],"title":"Relighting objects from image collections","type":"publication"},{"authors":["Thomas Kovac","Tom Haber","Frank van Reeth","Niel Hens"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"65afc8122d0078d2f553efc75d66825b","permalink":"https://tomhaber.github.io/publication/divergence/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/divergence/","section":"publication","summary":"Ordinary differential equations are widely used for the mathematical modeling of complex systems in biology and statistics. Since the analysis of such models needs to be performed using numerical integration, many applications can be gravely limited by the computational cost. This paper present a general-purpose integrator that runs massively parallel on graphics processing units. By minimizing thread divergence and bundling similar tasks using linear regression, execution time can be reduced by 40–80% when compared to a naive GPU implementation. Compared to a 36-core CPU implementation, a 150 fold runtime improvement is measured.","tags":["Pharmacometrics","Epidemiology","Parallelism","High-Performance Computing","Graphics Processing Units"],"title":"Improving ODE Integration on Graphics Processing Units by Reducing Thread Divergence","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}   Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://tomhaber.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"I’m interested in the implementation and design of high-performant, parallel software and algorithms. As well as, frameworks and domain-specific languages that enable expressing such software.\n","date":1540252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540252800,"objectID":"4ad9483811836389d75e55c8ef4ed7aa","permalink":"https://tomhaber.github.io/project/hpc/","publishdate":"2018-10-23T00:00:00Z","relpermalink":"/project/hpc/","section":"project","summary":"Software and language design for high-performance computing","tags":["High-Performance Computing","Parallel","Language design","Alogrithms"],"title":"High-performance Computing","type":"project"},{"authors":["Balazs Nemeth","Tom Haber","Jori Liesenborgs","Win Lamotte"],"categories":null,"content":"","date":153576e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":153576e4,"objectID":"d374f77da51572b25c509f7940d55473","permalink":"https://tomhaber.github.io/publication/specmcmc/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/publication/specmcmc/","section":"publication","summary":"  Sequential Monte Carlo methods are a useful tool to tackle non-linear problems in a Bayesian setting. A target posterior distribution is approximated by moving a set of weighted particles through a sequence of distributions. To counteract degeneracy caused by sequentially changing the underlying distribution, particles occasionally need to be resampled. Deciding if this is necessary requires a reduction operation on the weights after each update. Hence, scalability on a cluster is not only determined by the number of particles used, but also by how well load is balanced. This paper shows how speculative execution in Sequential Monte Carlo with Markov Chain Monte Carlo steps can improve parallel scalability. The key insight is that decisions taken based on the reduction result in each step can be accurately predicted. Consequently, synchronization inherent in the reduction can, in most cases, be avoided, relaxing the limit imposed by load imbalance. Particles are renumbered during resampling to further improve accuracy. Multiple test scenarios, each with different load balance characteristics, are studied empirically on a compute cluster. Tests show that when decisions are predicted correctly, execution time is reduced drastically for use cases with high load imbalance. Furthermore, the maximum theoretical gain, derived from execution characteristics, is compared with the measured improvement to verify that most speculative evaluations are actually useful. If predictions are incorrect, or load is balanced, speculation has no measurable negative impact. Performance is also evaluated in a weak scaling setting on cluster with 36 cores in each system.","tags":["Speculative Parallelism","Sequential Monte Carlo","High Performance Computing","Load Imbalance"],"title":"Relaxing Scalability Limits with Speculative Parallelism in Sequential Monte Carlo","type":"publication"},{"authors":null,"categories":null,"content":"Existing tools (NONMEM, SAS, Winbugs) are not well suited for complex models and high-performance. The algorithms used are not always very suitable for use with large numbers of parameters and complicated non-linear models. And more importantly the hardware landscape has changed tremendously since those tools were developed: single core chip performance has begun to stagnate with a resulting move to multicore and more parallelism.\nDiffMEM is tool for Non-Linear Mixed Effect model optimization in a frequentist and Bayesian context built specifically for complex models (mostly ordinary/partial differential equations) and performance. Through heavy the use of parallism, extensive low-level optimizations and algorithmic innovation, it is able to rapidly fit complex models. This in turn allows for shorter duty-cycles while building a model, and enables adaptive/optimal trial design and model validation since those typically require a lot of simulation+estimation steps.\n","date":1527033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527033600,"objectID":"4f6097ee2ef2646acbf769f1d7d1fdfa","permalink":"https://tomhaber.github.io/project/diffmem/","publishdate":"2018-05-23T00:00:00Z","relpermalink":"/project/diffmem/","section":"project","summary":"Scalable Differential equation based Mixed-Effects modelling","tags":["Non-linear","Mixed effects models","High-Performance Computing","Parallel"],"title":"DiffMEM","type":"project"},{"authors":["Thomas Kovac","Tom Haber","Frank van Reeth","Niel Hens"],"categories":null,"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"331360e332bf5217473b2cef4749dad8","permalink":"https://tomhaber.github.io/publication/heterogeneous/","publishdate":"2018-03-01T00:00:00Z","relpermalink":"/publication/heterogeneous/","section":"publication","summary":" ## Background\nOver the last years, substantial effort has been put into enhancing our arsenal in fighting epidemics from both technological and theoretical perspectives with scientists from different fields teaming up for rapid assessment of potentially urgent situations. This paper focusses on the computational aspects of infectious disease models and applies commonly available graphics processing units (GPUs) for the simulation of these models. However, fully utilizing the resources of both CPUs and GPUs requires a carefully balanced heterogeneous approach.\n## Results\nThe contribution of this paper is twofold. First, an efficient GPU implementation for evaluating a small-scale ODE model; here, the basic S(usceptible)-I(nfected)-R(ecovered) model, is discussed. Second, an asynchronous particle swarm optimization (PSO) implementation is proposed where batches of particles are sent asynchronously from the host (CPU) to the GPU for evaluation. The ultimate goal is to infer model parameters that enable the model to correctly describe observed data. The particles of the PSO algorithm are candidate parameters of the model; finding the right one is a matter of optimizing the likelihood function which quantifies how well the model describes the observed data. By employing a heterogeneous approach, in which both CPU and GPU are kept busy with useful work, speedups of 10 to 12 can be achieved on a moderate machine with a high-end consumer GPU as compared to a high-end system with 32 CPU cores.\n## Conclusions\nUtilizing GPUs for parameter inference can bring considerate increases in performance using average host systems with high-end consumer GPUs. Future studies should evaluate the benefit of using newer CPU and GPU architecture as well as applying this method to more complex epidemiological scenarios. ","tags":["ODE","PDE","Infectious diseases","Epidemiology","SIR model","GPU Asynchronous","Parallel","Particle Swarm Optimization","Heterogeneous computing"],"title":"Heterogeneous computing for epidemiological model fitting and simulation","type":"publication"},{"authors":["Valdemar Melicher","Tom Haber","Wim Vanroose"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"fd279d08dedf5584fc78e0e84127f14e","permalink":"https://tomhaber.github.io/publication/adjoint/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/adjoint/","section":"publication","summary":"We consider longitudinal data modeled by ordinary differential equations (ODEs), widespread models in physics, chemistry, biology and science in general. The sensitivity analysis of such dynamical systems usually requires calculation of various derivatives with respect to the model parameters. We employ the adjoint state method (ASM) for efficient computation of the first and the second derivatives of likelihood functionals constrained by ODEs. Essentially, the gradient can be computed with a cost (measured by model evaluations) that is independent of the number of the parameters and the Hessian with a linear cost in the number of the parameters instead of the quadratic one. The sensitivity analysis becomes feasible even if the parametric space is high-dimensional. In the theoretical part we rigorously study the ASM in the statistical context, when the discrete data are coupled with the continuous ODE model. Further, we present a highly optimized implementation of the results and its benchmarks on a number of problems.","tags":["Sensitivity Analysis","Ordinary Differential Equations","Gradient","Hessian","Statistical Computing","Mathematical Statistics","Algorithm"],"title":"Fast derivatives of likelihood functionals for ODE based models using adjoint-state method","type":"publication"},{"authors":["Jaak Simm","Adam Arany","Pooya Zakeri","Tom Haber","Joerg Kurt Wegner","Vladimir Chupakhin","Hugo Ceulemans","Yves Moreau"],"categories":null,"content":"","date":1504224e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224e3,"objectID":"935f40a9c6037d7e8110bfcdca9df6bf","permalink":"https://tomhaber.github.io/publication/macau/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/macau/","section":"publication","summary":"Bayesian matrix factorization is a method of choice for mak-ing  predictions  for  large-scale  incomplete  matrices,  due  to availability of efficient Gibbs sampling schemes and its robustness to overfitting.  In this paper, we consider factorization of large scale matrices with high-dimensional side information. However, sampling the link matrix for the side information with standard approaches costs O(F^3) time, where F is the dimensionality of the features.  To overcome this limitation we, firstly, propose a prior for the link matrix whose strength is proportional to the scale of latent variables.  Secondly,  using  this  prior  we  derive  an  efficient  sampler,  with linear complexity in the number of non-zeros, O(Nnz), by leveraging Krylov subspace methods, such as block conjugate gradient, allowing us to handle million-dimensional side in-formation. We demonstrate the effectiveness of our proposed method in drug-protein interaction prediction task","tags":["matrix  factorization","side  information","high scale machine learning","MCMC"],"title":"Macau: Scalable Bayesian factorization with high-dimensional side information using MCMC","type":"publication"},{"authors":["Petar Marendic","Jan Lemeire","Tom Haber","Dean Vucinic","Peter Schelkens"],"categories":null,"content":"","date":1343779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1343779200,"objectID":"7ff8997a8da4374023e558ffee7886ca","permalink":"https://tomhaber.github.io/publication/imbalance/","publishdate":"2012-08-01T00:00:00Z","relpermalink":"/publication/imbalance/","section":"publication","summary":"Today, most reduction algorithms are optimized for balanced workloads; they assume all processes will start the reduction at about the same time. However, in practice this is not always the case and significant load imbalances may occur and affect the performance of said algorithms. In this paper we investigate the impact of such imbalances on the most commonly employed reduction algorithms and propose a new algorithm specifically adapted to the presented context. Firstly, we analyze the optimistic case where we have a priori knowledge of all imbalances and propose a near-optimal solution. In the general case, where we do not have any foreknowledge of the imbalances, we propose a dynamically rebalanced tree reduction algorithm. We show experimentally that this algorithm performs better than the default OpenMPI and MVAPICH2 implementations.","tags":["MPI","imbalance","collective","reduction","process skew","benchmarking"],"title":"An Investigation into the Performance of Reduction Algorithms under Load Imbalance","type":"publication"},{"authors":["Tom Haber","Tom Mertens","Philippe Bekaert","Frank van Reeth"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"35809acd6126d58c23ad20b52bd9db3c","permalink":"https://tomhaber.github.io/publication/diffusion/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/diffusion/","section":"publication","summary":"To faithfully display objects consisting of translucent materials such as milk, fruit, wax and marble, one needs to take into account subsurface scattering of light. Accurate renderings require expensive simulation of light transport. Alternatively, the widely-used fast dipole approximation cannot deal with internal visibility issues, and has limited applicability (only homogeneous materials).We present a novel algorithm to plausibly reproduce subsurface scattering based on the diffusion approximation. This yields a relatively simple partial differential equation, which we propose to solve numerically using the multigrid method. The main difficulty in this approach consists of accurately representing interactions near the object s surface, for which we employ the embedded boundary discretization. Also, our method allows us to refine the simulation hierarchically where needed in order to optimize performance and memory usage. The resulting approach is capable of rapidly and accurately computing subsurface scattering in polygonal meshes for both homogeneous and heterogeneous materials. The amount of time spent computing subsurface scattering in a complex object is generally a few minutes.","tags":["Rendering","subsurface scattering","heterogeneous materials","embedded boundary","multigrid"],"title":"A computational approach to simulate light diffusion in arbitrarily shaped objects","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://tomhaber.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"Software Computational Software   DiffMEM: tool for Non-Linear Mixed Effect model optimization in a frequentist and Bayesian context built specifically for complex models (mostly ordinary/partial differential equations) and performance.\n  Severo.jl: Software package for scalable analysis and exploration of single-cell RNA-seq datasets\n  Various  PAPI.jl: Julia bindings for the PAPI hardware performance counters library and high-level interface MPIExecutor.jl: Parallel programming model with futures build on MPI  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6fcae98d7df3b6c44952e7b5fed181e3","permalink":"https://tomhaber.github.io/software/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/software/","section":"","summary":"Software Computational Software   DiffMEM: tool for Non-Linear Mixed Effect model optimization in a frequentist and Bayesian context built specifically for complex models (mostly ordinary/partial differential equations) and performance.","tags":null,"title":"","type":"page"}]